{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program 1\n",
      "Episodes: 1299\n",
      "Index(['Content'], dtype='object')\n",
      "\n",
      "0    還好天氣不錯\\n昨天晚上的流星雨\\n我看到很多流星\\n這次的收穫真豐富\\n當然豐富啦\\n我就...\n",
      "1    好熱喔\\n這種倉庫很不通風\\n好熱喔\\n受不了\\n今天天氣真的是太熱了\\n我都快中暑了\\n那...\n",
      "Name: Content, dtype: object\n",
      "\n",
      "Program 2\n",
      "Episodes: 205\n",
      "Index(['Content'], dtype='object')\n",
      "\n",
      "0    我們現在只差兩分\\n只差兩分\\n等下阿偉先站過來\\n他們會埋伏一個射手出來\\n我們盡量把他堵...\n",
      "1    四十年前\\n我媽為了養我跟我哥\\n開這間理髮店\\n她把手藝都傳給哥\\n希望他可以接下這間店\\...\n",
      "Name: Content, dtype: object\n",
      "\n",
      "Program 3\n",
      "Episodes: 57\n",
      "Index(['Content'], dtype='object')\n",
      "\n",
      "0    台南人劇團\\n一個從古都台南輸出的\\n現代劇團\\n每年總有令人驚奇的戲劇產生\\n玩大師\\n莎...\n",
      "1    一齣舞台劇\\n這個舞台劇的結果\\n所有觀眾都知道\\n兩個演員在舞台上\\n撐了一百多分鐘\\n目...\n",
      "Name: Content, dtype: object\n",
      "\n",
      "Program 4\n",
      "Episodes: 10\n",
      "Index(['Content'], dtype='object')\n",
      "\n",
      "0    念書幹嘛偷光\\n燈一開就有了啊\\n太陽是不是從樹葉之間的\\n這個縫灑下來\\n然後在地上啊\\n...\n",
      "1    倫語社\\n效果立竿見影\\n立竿見影\\n這也是指很快囉\\n但是它和曇花一現\\n有什麼不一樣\\n...\n",
      "Name: Content, dtype: object\n",
      "\n",
      "Program 5\n",
      "Episodes: 369\n",
      "Index(['Content'], dtype='object')\n",
      "\n",
      "0    公平的對待\\n孩子才會樂於做良性的競爭\\n這樣一來\\n真正有實力的人才不會被埋沒\\n老師好\\...\n",
      "1    你們看\\n我臉上的痘痘\\n畫了粧之後就沒那麼明顯了\\n就算熬夜K書\\n長了黑眼圈也不怕\\n你...\n",
      "Name: Content, dtype: object\n",
      "\n",
      "Program 6\n",
      "Episodes: 80\n",
      "Index(['Content'], dtype='object')\n",
      "\n",
      "0    在這個世上\\n既能解放你滿肚子壓力\\n又讓你避之唯恐不及的\\n只有馬桶\\n但是如果你到現在還...\n",
      "1    你相信嗎\\n全球十大致人於死的動物榜首是誰\\n獅子嗎\\n不是\\n鱷魚嗎\\nNo\\n答案竟然是...\n",
      "Name: Content, dtype: object\n",
      "\n",
      "Program 7\n",
      "Episodes: 611\n",
      "Index(['Content'], dtype='object')\n",
      "\n",
      "0    嗨, 大家好\\n歡迎收看「聽聽看」\\n你這個禮拜過得好不好呢\\n有沒有什麼新鮮事\\n要和朋友...\n",
      "1    你今天是不是跟我一樣\\n早就迫不及待的\\n想要收看我們「聽聽看」了呢\\n怎麼樣\\n上個禮拜的...\n",
      "Name: Content, dtype: object\n",
      "\n",
      "Program 8\n",
      "Episodes: 210\n",
      "Index(['Content'], dtype='object')\n",
      "\n",
      "0    每天帶你拜訪一個家庭\\n邀請一位貴賓和他們共進晚餐\\n談談人生大小事\\n但如果登門拜訪的\\n...\n",
      "1    如果用一句話\\n來形容吃飯這件事情\\n那句話應該就是體驗人生\\n今天的「誰來晚餐」\\n發生在...\n",
      "Name: Content, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "# import pandas as pd\n",
    "\n",
    "# NUM_PROGRAM = 8\n",
    "# programs = []\n",
    "# for i in range(1, NUM_PROGRAM+1):\n",
    "#     program = pd.read_csv('Program0%d.csv' % (i))\n",
    "    \n",
    "#     print('Program %d' % (i))\n",
    "#     print('Episodes: %d' % (len(program)))\n",
    "#     print(program.columns)\n",
    "#     print()\n",
    "    \n",
    "#     print(program.loc[:1]['Content'])\n",
    "#     print()\n",
    "    \n",
    "#     programs.append(program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question\n",
      "Episodes: 500\n",
      "Index(['Question', 'Option0', 'Option1', 'Option2', 'Option3', 'Option4',\n",
      "       'Option5'],\n",
      "      dtype='object')\n",
      "\n",
      "0    媽給你送錢包來啦 來 你看一下是不是這個\\n對 就是這個 你在哪裡找到它的\\n\n",
      "1             古人說三日不讀書 面目可憎 我覺得我最近可能臉色太難看了\\n\n",
      "2                         你說我們做父母的最擔心的就是這個\\n\n",
      "Name: Question, dtype: object\n",
      "\n",
      "0        你看 這是我新買的錢包\n",
      "1    所以想回復我昔日面貌姣好的樣子\n",
      "2      我剛剛聽你媽說你要讀什麼科\n",
      "Name: Option0, dtype: object\n",
      "\n",
      "0     我的錢包不見了啦\n",
      "1    是不是要定期來舉辦\n",
      "2    其他老師又集體叛變\n",
      "Name: Option1, dtype: object\n",
      "\n",
      "0    以後上網咖的錢包在我身上\n",
      "1         各辦理一次才對\n",
      "2        聽起來好好玩天啊\n",
      "Name: Option2, dtype: object\n",
      "\n",
      "0                          什麼有錢包場\n",
      "1                     能夠督促所有的用人機關\n",
      "2    只是小孩自己的興趣不能得到發展 他們的心裡可能也會很悶喔\n",
      "Name: Option3, dtype: object\n",
      "\n",
      "0    早上你爸爸在車上找到的 一定是前天你放學的時候掉在車上了\n",
      "1                   在上次的節目討論中也有提到\n",
      "2                      走到這裡就沒有路了耶\n",
      "Name: Option4, dtype: object\n",
      "\n",
      "0         我為什麼要給你們錢包\n",
      "1           超過九十分貝以上\n",
      "2    每一個科目像是國語數學都很優秀\n",
      "Name: Option5, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "questions = pd.read_csv('Question.csv')\n",
    "\n",
    "print('Question')\n",
    "print('Episodes: %d' % (len(questions)))\n",
    "print(questions.columns)\n",
    "print()\n",
    "\n",
    "print(questions.loc[:2]['Question'])\n",
    "print()\n",
    "\n",
    "for i in range(6):\n",
    "    print(questions.loc[:2]['Option%d' % (i)])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\a1989\\Desktop\\DL\\Comp1\\big5_dict.txt ...\n",
      "Loading model from cache C:\\Users\\a1989\\AppData\\Local\\Temp\\jieba.u13ee1a1953b639ea7ba618182ba7d2dd.cache\n",
      "Loading model cost 1.746 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '討厭', '吃', '蘋果']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "jieba.set_dictionary('big5_dict.txt')\n",
    "example_str = '我討厭吃蘋果'\n",
    "cut_example_str = jieba.lcut(example_str)\n",
    "print(cut_example_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jieba_lines(lines):\n",
    "    cut_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        cut_line = jieba.lcut(line)\n",
    "        cut_lines.append(cut_line)\n",
    "    \n",
    "    return cut_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut_programs = []\n",
    "\n",
    "# for program in programs:\n",
    "#     episodes = len(program)\n",
    "#     cut_program = []\n",
    "    \n",
    "#     for episode in range(episodes):\n",
    "#         lines = program.loc[episode]['Content'].split('\\n')\n",
    "#         cut_program.append(jieba_lines(lines))\n",
    "    \n",
    "#     cut_programs.append(cut_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # with open('cut.pkl', 'wb') as f:\n",
    "# #     pickle.dump(cut_programs, f)\n",
    "# with open('cut.pkl', 'rb') as f:\n",
    "#     cut_programs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 programs\n",
      "1299 episodes in program 0\n",
      "635 lines in first episode of program 0\n",
      "\n",
      "first 3 lines in 1st episode of program 0: \n",
      "[['還好', '天氣', '不錯'], ['昨天', '晚上', '的', '流星雨'], ['我', '看到', '很多', '流星']]\n"
     ]
    }
   ],
   "source": [
    "print(\"%d programs\" % len(cut_programs))\n",
    "print(\"%d episodes in program 0\" % len(cut_programs[0]))\n",
    "print(\"%d lines in first episode of program 0\" % len(cut_programs[0][0]))\n",
    "\n",
    "print()\n",
    "print(\"first 3 lines in 1st episode of program 0: \")\n",
    "print(cut_programs[0][0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_questions = []\n",
    "n = len(questions)\n",
    "\n",
    "for i in range(n):\n",
    "    cut_question = []\n",
    "    lines = questions.loc[i]['Question'].split('\\n')\n",
    "    cut_question.append(jieba_lines(lines))\n",
    "    \n",
    "    for j in range(6):\n",
    "        line = questions.loc[i]['Option%d' % (j)]\n",
    "        cut_question.append(jieba.lcut(line))\n",
    "    \n",
    "    cut_questions.append(cut_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 questions\n",
      "7\n",
      "[['媽給', '你', '送', '錢包', '來', '啦', ' ', '來', ' ', '你', '看', '一下', '是', '不', '是', '這個'], ['對', ' ', '就是', '這個', ' ', '你', '在', '哪裡', '找到', '它', '的'], []]\n",
      "['你', '看', ' ', '這是', '我', '新', '買', '的', '錢包']\n",
      "['我', '的', '錢包', '不見了', '啦']\n",
      "['以後', '上', '網咖', '的', '錢包', '在', '我', '身上']\n",
      "['什麼', '有', '錢包', '場']\n",
      "['早上', '你', '爸爸', '在', '車上', '找到', '的', ' ', '一定', '是', '前天', '你', '放學', '的', '時候', '掉', '在', '車上', '了']\n",
      "['我', '為什麼', '要給', '你們', '錢包']\n"
     ]
    }
   ],
   "source": [
    "print(\"%d questions\" % len(cut_questions))\n",
    "print(len(cut_questions[0]))\n",
    "\n",
    "# 1 question\n",
    "print(cut_questions[0][0])\n",
    "\n",
    "# 6 optional reponses\n",
    "for i in range(1, 7):\n",
    "    print(cut_questions[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# np.save('cut_Programs.npy', cut_programs)\n",
    "# np.save('cut_Questions.npy', cut_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_programs = np.load('cut_Programs.npy')\n",
    "cut_Question = np.load('cut_Questions.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 174420 words in word_dict\n"
     ]
    }
   ],
   "source": [
    "word_dict = dict()\n",
    "def add_word_dict(w):\n",
    "    if not w in word_dict:\n",
    "        word_dict[w] = 1\n",
    "    else:\n",
    "        word_dict[w] += 1\n",
    "for program in cut_programs:\n",
    "    for lines in program:\n",
    "        for line in lines:\n",
    "            for w in line:\n",
    "                add_word_dict(w)\n",
    "for question in cut_questions:\n",
    "    lines = question[0]\n",
    "    for line in lines:\n",
    "        for w in line:\n",
    "            add_word_dict(w)\n",
    "    \n",
    "    for i in range(1, 7):\n",
    "        line = question[i]\n",
    "        for w in line:\n",
    "            add_word_dict(w)\n",
    "import operator\n",
    "\n",
    "word_dict = sorted(word_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(\"Total %d words in word_dict\" % len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('他', 81632), ('也', 77206), ('就是', 75579), ('說', 74281), ('來', 68735), ('會', 67933), ('那', 66863), ('喔', 61530), ('可以', 60268), ('跟', 60056)]\n",
      "\n",
      "Total 15000 words in voc_dict\n"
     ]
    }
   ],
   "source": [
    "VOC_SIZE = 15000\n",
    "VOC_START = 20\n",
    "\n",
    "voc_dict = word_dict[VOC_START:VOC_START+VOC_SIZE]\n",
    "print(voc_dict[:10])\n",
    "print()\n",
    "print(\"Total %d words in voc_dict\" % len(voc_dict))\n",
    "np.save('voc_dict.npy', voc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_dict = np.load('voc_dict.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "NUM_PROGRAM = 8\n",
    "NUM_TRAIN = 10000\n",
    "TRAIN_VALID_RATE = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data():\n",
    "    Xs, Ys = [], []\n",
    "    \n",
    "    for i in range(NUM_TRAIN):\n",
    "        pos_or_neg = random.randint(0, 1)\n",
    "        \n",
    "        if pos_or_neg==1:\n",
    "            program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            episode_id = random.randint(0, len(cut_programs[program_id])-1)\n",
    "            line_id = random.randint(0, len(cut_programs[program_id][episode_id])-2)\n",
    "            \n",
    "            Xs.append([cut_programs[program_id][episode_id][line_id], \n",
    "                       cut_programs[program_id][episode_id][line_id+1]])\n",
    "            Ys.append(1)\n",
    "            \n",
    "        else:\n",
    "            first_program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            first_episode_id = random.randint(0, len(cut_programs[first_program_id])-1)\n",
    "            first_line_id = random.randint(0, len(cut_programs[first_program_id][first_episode_id])-1)\n",
    "            \n",
    "            second_program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            second_episode_id = random.randint(0, len(cut_programs[second_program_id])-1)\n",
    "            second_line_id = random.randint(0, len(cut_programs[second_program_id][second_episode_id])-1)\n",
    "            \n",
    "            Xs.append([cut_programs[first_program_id][first_episode_id][first_line_id], \n",
    "                       cut_programs[second_program_id][second_episode_id][second_line_id]])\n",
    "            Ys.append(0)\n",
    "    \n",
    "    return Xs, Ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xs, Ys = generate_training_data()\n",
    "\n",
    "x_train, y_train = Xs[:int(NUM_TRAIN*TRAIN_VALID_RATE)], Ys[:int(NUM_TRAIN*TRAIN_VALID_RATE)]\n",
    "x_valid, y_valid = Xs[int(NUM_TRAIN*TRAIN_VALID_RATE):], Ys[int(NUM_TRAIN*TRAIN_VALID_RATE):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['還好 天氣 不錯 ', '昨天 晚上 ', '看到 很多 流星 ', '這次 收穫 真 豐富 ', '當然 豐富 啦 ', '說 嘛 ', '精心 製作 ', '被 一個 人 吃掉 ', '真的 嗎 ', '不要 忘記 做 秘密 檔案 ']\n"
     ]
    }
   ],
   "source": [
    "example_doc = []\n",
    "\n",
    "# lines in 1st episode in program 0 \n",
    "for line in cut_programs[0][0]:\n",
    "    example_line = ''\n",
    "    for w in line:\n",
    "        if w in voc_dict:\n",
    "            example_line += w+' '\n",
    "        \n",
    "    example_doc.append(example_line)\n",
    "\n",
    "print(example_doc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabulary]\n",
      "\n",
      "還好 469\n",
      "天氣 168\n",
      "不錯 46\n",
      "昨天 268\n",
      "晚上 270\n",
      "看到 352\n",
      "很多 217\n",
      "流星 310\n",
      "這次 456\n",
      "收穫 259\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ngram_range=(min, max), default: 1-gram => (1, 1)\n",
    "count = CountVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "count.fit(example_doc)\n",
    "BoW = count.vocabulary_\n",
    "print('[vocabulary]\\n')\n",
    "for key in list(BoW.keys())[:10]:\n",
    "    print('%s %d' % (key, BoW[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (lid, vid)     tf\n",
      "  (0, 46)\t1\n",
      "  (0, 168)\t1\n",
      "  (0, 469)\t1\n",
      "  (1, 268)\t1\n",
      "  (1, 270)\t1\n",
      "  (2, 217)\t1\n",
      "  (2, 310)\t1\n",
      "  (2, 352)\t1\n",
      "  (3, 259)\t1\n",
      "  (3, 435)\t1\n",
      "  (3, 456)\t1\n",
      "  (4, 340)\t1\n",
      "  (4, 435)\t1\n",
      "  (6, 370)\t1\n",
      "  (6, 414)\t1\n",
      "  (7, 6)\t1\n",
      "  (7, 128)\t1\n",
      "  (8, 354)\t1\n",
      "  (9, 44)\t1\n",
      "  (9, 225)\t1\n",
      "  (9, 293)\t1\n",
      "  (9, 361)\t1\n",
      "\n",
      "Is document-term matrix a scipy.sparse matrix? True\n"
     ]
    }
   ],
   "source": [
    "# get matrix (line_id, vocabulary_id) --> (this vocabulary occurs how many times in this line)\n",
    "doc_bag = count.transform(example_doc)\n",
    "print(' (lid, vid)     tf')\n",
    "print(doc_bag[:10])\n",
    "\n",
    "print('\\nIs document-term matrix a scipy.sparse matrix? {}'.format(sp.sparse.issparse(doc_bag)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "After calling .toarray(), is it a scipy.sparse matrix? False\n"
     ]
    }
   ],
   "source": [
    "doc_bag = doc_bag.toarray()\n",
    "print(doc_bag[:10])\n",
    "\n",
    "print('\\nAfter calling .toarray(), is it a scipy.sparse matrix? {}'.format(sp.sparse.issparse(doc_bag)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[most frequent vocabularies]\n",
      "蟋蟀: 98\n",
      "可以: 21\n",
      "就是: 21\n",
      "聲音: 20\n",
      "這樣: 19\n",
      "你們: 17\n",
      "真的: 16\n",
      "還有: 15\n",
      "比較: 15\n",
      "豆油伯: 15\n"
     ]
    }
   ],
   "source": [
    "doc_bag = count.fit_transform(example_doc).toarray()\n",
    "\n",
    "print(\"[most frequent vocabularies]\")\n",
    "\n",
    "# conpute how many times each word occurs in doc_bag\n",
    "bag_cnts = np.sum(doc_bag, axis=0)\n",
    "\n",
    "# get words occur in doc_bag\n",
    "words_num = bag_cnts.shape[0]\n",
    "ones_vector = np.ones(words_num)\n",
    "words = count.inverse_transform(ones_vector)[0]\n",
    "\n",
    "# sort bag_cnts and get its indices and values\n",
    "most_freq_word_index = bag_cnts.argsort()\n",
    "most_freq_word_times = np.sort(bag_cnts)\n",
    "\n",
    "top = 10\n",
    "# [::-1] reverses a list since sort is in ascending order\n",
    "for tok, v in zip(words[most_freq_word_index[::-1][:top]], \n",
    "                        most_freq_word_times[::-1][:top]):\n",
    "    print('%s: %d' % (tok, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabularies with smallest idf scores]\n",
      "蟋蟀: 2.87\n",
      "可以: 4.36\n",
      "就是: 4.41\n",
      "聲音: 4.46\n",
      "這樣: 4.46\n",
      "你們: 4.56\n",
      "真的: 4.62\n",
      "還有: 4.68\n",
      "豆油伯: 4.68\n",
      "比較: 4.68\n",
      "\n",
      "[vocabularies with highest tf-idf scores]\n",
      "蟋蟀: 42.02\n",
      "這樣: 11.92\n",
      "真的: 11.41\n",
      "就是: 11.26\n",
      "可以: 10.90\n",
      "聲音: 10.44\n",
      "豆油伯: 10.33\n",
      "還有: 9.84\n",
      "你們: 9.29\n",
      "叫做: 8.40\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,1))\n",
    "tfidf.fit(example_doc)\n",
    "\n",
    "top = 10\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "for i in range(top):\n",
    "    print('%s: %.2f' % (tfidf.get_feature_names()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "doc_tfidf = tfidf.transform(example_doc).toarray()\n",
    "tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "for tok, v in zip(tfidf.inverse_transform(np.ones(tfidf_sum.shape[0]))[0][tfidf_sum.argsort()[::-1]][:top], \n",
    "                  np.sort(tfidf_sum)[::-1][:top]):\n",
    "    print('%s: %.2f' % (tok, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(635, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hashvec = HashingVectorizer(n_features=2**6)\n",
    "\n",
    "doc_hash = hashvec.transform(example_doc)\n",
    "print(doc_hash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  This movie is well done on so many levels that...          1\n",
      "1  Wilson (Erica Gavin) is nabbed by the cops and...          1\n",
      "2  Canto 1: How Kriemhild Mourned Over Siegfried ...          1\n",
      "3  I bought Bloodsuckers on ebay a while ago. I w...          0\n",
      "4  I took part in a little mini production of thi...          1\n",
      "5  This is certainly one of my all time fav episo...          1\n",
      "6  This scary and rather gory adaptation of Steph...          1\n",
      "7  Mike Hawthorne(Gordon Currie)is witness to the...          0\n",
      "8  It looks to me as if the creators of \"The Clas...          0\n",
      "9  This comic book style film is funny, has nicel...          1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk\n",
    "\n",
    "print(next(get_stream(path='imdb.csv', size=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello this is a sanity check  :( ;P\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "print(preprocessor('<a href=\"example.com\">Hello, This :-( is a sanity check ;P!</a>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\a1989\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "['runner', 'like', 'run', 'thu', 'run']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# Stop Words are words which do not contain important significance to be used in Search Queries. i.e. the, and,...etc\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "print(tokenizer_stem_nostop('runners like running and thus they run'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000/4000] 0.877372\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "hashvec = HashingVectorizer(n_features=2**20, \n",
    "                            preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "# loss='log' gives logistic regression\n",
    "clf = SGDClassifier(loss='log', max_iter=100)\n",
    "\n",
    "batch_size = 1000\n",
    "stream = get_stream(path='imdb.csv', size=batch_size)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "train_auc, val_auc = [], []\n",
    "\n",
    "# we use one batch for training and another for validation in each iteration\n",
    "iters = int(2000/(batch_size*2))\n",
    "\n",
    "for i in range(iters):\n",
    "    batch = next(stream)\n",
    "    X_train, y_train = batch['review'], batch['sentiment']\n",
    "    if X_train is None:\n",
    "        break\n",
    "        \n",
    "    X_train = hashvec.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    train_auc.append(roc_auc_score(y_train, clf.predict_proba(X_train)[:,1]))\n",
    "    \n",
    "    # validate\n",
    "    batch = next(stream)\n",
    "    X_val, y_val = batch['review'], batch['sentiment']\n",
    "    \n",
    "    X_val = hashvec.transform(X_val)\n",
    "    score = roc_auc_score(y_val, clf.predict_proba(X_val)[:,1])\n",
    "    val_auc.append(score)\n",
    "    print('[%d/%d] %f' % ((i+1)*(batch_size*2), 4000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(train_auc)+1), train_auc, color='blue', label='Train auc')\n",
    "plt.plot(range(1, len(train_auc)+1), val_auc, color='red', label='Val auc')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('#Batches')\n",
    "plt.ylabel('Auc')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 0.881\n"
     ]
    }
   ],
   "source": [
    "# import optimized pickle written in C for serializing and  de-serializing a Python object\n",
    "import _pickle as pkl\n",
    "\n",
    "# dump to disk\n",
    "pkl.dump(hashvec, open('hashvec.pkl', 'wb'))\n",
    "pkl.dump(clf, open('clf-sgd.pkl', 'wb'))\n",
    "\n",
    "# load from disk\n",
    "hashvec = pkl.load(open('hashvec.pkl', 'rb'))\n",
    "clf = pkl.load(open('clf-sgd.pkl', 'rb'))\n",
    "\n",
    "df_test = pd.read_csv('imdb.csv')\n",
    "print('test auc: %.3f' % roc_auc_score(df_test['sentiment'], \n",
    "                                       clf.predict_proba(hashvec.transform(df_test['review']))[:,1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
